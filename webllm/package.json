{
  "name": "llama-webllm-demo",
  "version": "1.0.0",
  "description": "Llama-3.2-1B-Instruct running with WebLLM",
  "main": "index.html",
  "scripts": {
    "start": "npx http-server . -p 8080 -c-1",
    "build": "npm install",
    "serve": "npx http-server . -p 8080 -c-1 -o"
  },
  "dependencies": {
    "@mlc-ai/web-llm": "^0.2.46"
  },
  "devDependencies": {
    "http-server": "^14.1.1"
  },
  "keywords": [
    "llama",
    "webllm",
    "mlc",
    "ai",
    "inference"
  ],
  "author": "User",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "local"
  }
}